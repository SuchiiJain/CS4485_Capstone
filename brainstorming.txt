Documentation Rot Detector ("Docrot Detector") - Brainstorming Notes

Date: February 19, 2026

Project Idea
- Build a tool that detects when code has changed in meaningful ways and flags linked documentation for review.
- Goal: prevent documentation rot at the source, especially in fast-moving repos.

Problem It Solves
- Documentation gets stale as code evolves.
- Outdated docs slow onboarding and increase engineering/training costs.
- Open source projects and companies both suffer from this.

Who Would Use It
- Developers
- Technical writers

Core MVP Behavior
- On push (or CI run), analyze code changes.
- Detect whether logic changed in a substantial way.
- If yes, flag mapped documentation (e.g., README, module docs, API docs) with a reminder to review/update.

High-Level MVP Architecture
1) Mapping Layer
- Map source paths/modules to documentation files via config (simple JSON/YAML for MVP).

2) Change Detection Layer
- Parse old and new code snapshots using AST.
- Generate semantic fingerprints per function/method.
- Compare fingerprints and compute a semantic change score.

3) Persistence Layer
- Store previous fingerprints in a JSON file (.docrot-fingerprints.json) for MVP.
- This file is the single source of truth for "old" state — on each run, we compare
  stored fingerprints against freshly-extracted fingerprints from the current code.
  We do NOT re-parse old code from a git ref; the stored file IS the old baseline.
- Post-MVP: migrate to SQLite for indexed queries and large-codebase support.

4) Alert Layer
- Emit warning/status when mapped docs may be stale.


Brainstorm Focus: How to Detect "Substantial Logic Change"

Key Principle
- Use semantic AST deltas, not raw text diffs.
- Ignore formatting-only and comment-only changes.

Normalization (ignore noise)
- Strip/ignore:
	- whitespace/formatting changes
	- comments/docstrings-only edits
	- import ordering changes
	- local variable renames (where safely detectable)

Semantic Fingerprint (per function/method)
- Function signature (name, params, defaults, return annotation)
- Control-flow shape (if/elif/else, loop structure, early returns)
- Operators/conditions used in branching
- Calls to external APIs/services (db/file/network/auth)
- Exception behavior (raised/caught/propagated)
- State mutation signals (writes to files/db/global/shared state)

Scoring Model (starter MVP weights)
- 0 points:
	- comments/formatting only
	- docstring-only change
	- local rename only

- 1 point:
	- literal/constant tweak (e.g., 5 -> 10)
	- default argument tweak
	- reordering non-exported private helpers

- 3 points:
	- condition expression changed
	- loop semantics changed
	- return expression/branch outcome changed

- 5 points:
	- public function signature changed
	- exported/public API added/removed/renamed

- 6 points:
	- side-effect behavior changed (db/file/network/auth call add/remove)
	- permission/auth logic changed

- 8 points:
	- exception behavior changed significantly
	- core control path added/removed

Critical Event Triggers (always flag)
- Public API contract changes
- New/removed side effects (DB/file/network)
- Auth/permission logic changes
- Exception behavior changed significantly (e.g., swallowing exceptions, new raises)
- Core control path added/removed

Flagging Rules (MVP)
- Per function: substantial if score >= 4 OR critical event present.
- Per push/repo area:
	- flag docs if any critical event occurs, OR
	- cumulative semantic score for code mapped to a doc file >= 8.

Why This Should Work
- Avoids false positives from formatting/comments.
- Captures meaningful behavioral changes.
- Gives tunable thresholds for teams with different sensitivity.

MVP Scope Choices (recommended)
- Start with one language (Python for MVP).
- Start with config-file mapping only (skip dashboard for MVP).
- Run in CI only for MVP (simplest rollout), with optional pre-push hook later.

Beginner AST Primer (Python)
- AST = Abstract Syntax Tree: Python source is parsed into a tree of node types representing code structure.
- This is ideal for MVP because it compares structure/logic instead of plain text.

What AST naturally ignores
- Whitespace and indentation style differences.
- Most formatting-only changes.
- Comments are not represented as normal executable AST nodes.

What AST captures clearly
- Function definitions and signatures (`FunctionDef`, arguments/defaults/annotations).
- Conditions/branch logic (`If`, `Compare`, operators like `Gt`, `GtE`, `Eq`).
- Loops (`For`, `While`), returns (`Return`), exceptions (`Raise`, `Try`).
- Function calls and expression structure (`Call`, `Attribute`, etc.).

How this maps to our "substantial change" logic
- Comment/whitespace-only edits:
	- AST shape remains effectively unchanged -> score 0.

- Constant tweak:
	- Example: `MAX_RETRIES = 3` -> `MAX_RETRIES = 5`.
	- AST difference appears in `Constant(value=...)` node -> minor score (e.g., +1).

- Condition expression change:
	- Example: `if x > 10` -> `if x >= 10`.
	- AST difference appears in comparison operator (`Gt` -> `GtE`) -> medium score (e.g., +3).

- API contract change:
	- Example: `def fetch(user_id)` -> `def fetch(user_id, include_deleted=False)`.
	- AST difference appears in function args/defaults/signature -> high score (e.g., +5) and critical trigger.

Docstring handling note
- Python docstrings are represented as a leading string expression in module/class/function bodies.
- We can explicitly detect and ignore docstring-only diffs so they do not trigger substantial-change flags.

Practical caveat (for team discussion)
- AST is very strong for syntax/structure-level semantics, but not a full runtime behavior proof.
- For MVP doc-rot detection, AST + weighted scoring is still a practical and credible approach.

Potential Non-MVP Enhancements
- SQLite fingerprint storage (replace JSON; better for large codebases and analytics).
- Per-module/doc configurable thresholds (override global defaults for high/low-risk areas).
- Dashboard: history of flagged docs, pending review status.
- Repo analytics: most stale modules/docs.
- LLM-assisted suggested doc updates tied to detected semantic changes.

Decisions Made
1) Fingerprint storage: JSON file for MVP (.docrot-fingerprints.json in repo). Upgrade to SQLite post-MVP.
2) Thresholds: Global for MVP (single per_function and per_doc value). Per-module overrides post-MVP.

Open MVP Questions

1) Alert delivery — where do flagged warnings appear?
	- Option A: CI log output only (just print warnings).
	- Option B: Generate a report file (.docrot-report.json) as a CI artifact.
	- Option C: Post a PR comment via GitHub API.
	- Recommendation: A + B for MVP (log warnings AND write a report artifact). PR comments post-MVP.

2) Config format — JSON or YAML for doc mappings?
	- JSON: no extra dependency, but less human-friendly.
	- YAML: more readable, but requires pyyaml dependency.
	- Recommendation: JSON for MVP (zero dependencies, consistent with fingerprint file format).

3) Initial baseline — what happens on first run?
	- No prior fingerprints exist yet. First run could flag everything or silently generate a baseline.
	- Recommendation: First run = generate baseline only, zero alerts. Flag on subsequent changes only.
	- IMPORTANT: This means on first run, new-file flagging (see #4) does NOT apply.
	  The first run is purely a "learn the codebase" step. New/deleted file flagging
	  only triggers on the second and subsequent runs, when a real baseline exists.

4) New files and deleted files — how to handle?
	- New .py file added (after baseline exists): flag mapped docs. New code likely needs doc coverage.
	- .py file deleted entirely: flag mapped docs. Docs may reference removed functionality.
	- Recommendation: Flag both for MVP. Better to over-alert than miss a stale doc.
	- NOTE: On first run (no prior baseline), these do NOT trigger alerts — see #3 above.

5) Class-level and module-level code
	- Class methods: already covered (treated as functions).
	- Class attribute changes, inheritance changes, module-level executable code: additional work.
	- Recommendation: Defer class attributes, inheritance, and module-level code to post-MVP. Methods are covered.

6) What counts as "documentation"?
	- External doc files only (.md, .rst, .txt)?
	- Or also inline docstrings within Python files?
	- Recommendation: External files only for MVP. Inline docstring staleness is a related but separate problem.

7) CI platform target
	- GitHub Actions specifically? Or CI-agnostic CLI script?
	- Recommendation: CLI-first (a Python script/command that CI calls), with a sample GitHub Actions workflow.
		Keeps it portable and easier to demo locally too.

8) Testing / demo strategy
	- Need a sample repo with known before/after commits covering each scoring tier.
	- This doubles as capstone demo material.
	- Recommendation: Build this early. Create a small test repo with commits for:
		- formatting-only change (should NOT flag)
		- constant tweak (should NOT flag alone)
		- condition change (should flag)
		- API signature change (should flag, critical)
		- side-effect change (should flag, critical)
		Use it for automated tests AND live demo.

Known Limitations / Caveats (to address post-MVP)

1) Renamed/moved functions cause false positives.
	- If a function moves from file A to file B, it appears as "removed" in A (score 5)
	  and "added" in B (score 5). Both map to their respective docs, so the cumulative
	  score inflates for what may be a no-op refactor.
	- Mitigation (post-MVP): Use signature-similarity matching to detect likely moves
	  and suppress duplicate alerts.

2) The early-return for "format/comment only" in score_semantic_delta must be carefully
   implemented. If a commit changes both formatting AND logic in the same function,
   the logic change must not be masked by the formatting flag. The implementation
   should only return 0 if ALL detected changes are non-semantic.
